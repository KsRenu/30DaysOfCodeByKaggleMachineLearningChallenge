{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. \r\n",
    "# It generally has much better predictive accuracy than a single decision tree and it works well with default parameters. \r\n",
    "# If you keep modeling, you can learn more models with even better performance,\r\n",
    "#  but many of those are sensitive to getting the right parameters."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "    \r\n",
    "# Load data\r\n",
    "melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'\r\n",
    "melbourne_data = pd.read_csv(melbourne_file_path) \r\n",
    "# Filter rows with missing values\r\n",
    "melbourne_data = melbourne_data.dropna(axis=0)\r\n",
    "# Choose target and features\r\n",
    "y = melbourne_data.Price\r\n",
    "melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \r\n",
    "                        'YearBuilt', 'Lattitude', 'Longtitude']\r\n",
    "X = melbourne_data[melbourne_features]\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "# split data into training and validation data, for both features and target\r\n",
    "# The split is based on a random number generator. Supplying a numeric value to\r\n",
    "# the random_state argument guarantees we get the same split every time we\r\n",
    "# run this script.\r\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We build a random forest model similarly to how we built a decision tree in scikit-learn - \r\n",
    "# this time using the RandomForestRegressor class instead of DecisionTreeRegressor.\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "from sklearn.metrics import mean_absolute_error\r\n",
    "\r\n",
    "forest_model = RandomForestRegressor(random_state=1)\r\n",
    "forest_model.fit(train_X, train_y)\r\n",
    "melb_preds = forest_model.predict(val_X)\r\n",
    "print(mean_absolute_error(val_y, melb_preds))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# But one of the best features of Random Forest models is that they generally work reasonably even without this tuning."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercises\r\n",
    "Data science isn't always this easy. But replacing the decision tree with a Random Forest is going to be an easy win."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Use a Random Forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "\r\n",
    "# Define the model. Set random_state to 1\r\n",
    "rf_model = RandomForestRegressor()\r\n",
    "\r\n",
    "# fit your model\r\n",
    "rf_model.fit(train_X, train_y)\r\n",
    "\r\n",
    "# Calculate the mean absolute error of your Random Forest model on the validation data\r\n",
    "rf_val_predictions = rf_model.predict(val_X)\r\n",
    "rf_val_mae = mean_absolute_error(rf_val_predictions, val_y)\r\n",
    "\r\n",
    "print(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}